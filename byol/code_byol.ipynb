{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVFdiclPmxNH"
      },
      "source": [
        "# BYOL in PyTorch\n",
        "\n",
        "This is written by Ying Liu (yili@di.ku.dk) who is the student in the Department of Computer Science at KÃ¸benhavns Universitet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnJUKxxNxcC6"
      },
      "source": [
        "## Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb5YVPldvmc1"
      },
      "source": [
        "<div style=\"display: flex; justify-content: center; align-items: center; background: white;\">\n",
        "    <img src=\"https://imgur.com/rBZRkOf.png\" style=\"width: 20%; margin-right: 1rem;\" />\n",
        "    <img src=\"https://imgur.com/uudkfAk.png\" style=\"width: 20%;\" />\n",
        "</div>\n",
        "\n",
        "\n",
        "There are three important components, they are **encoder** $f$ (ResNet-50), **projector** $g$ and **predictor** $q$, respectively.\n",
        "\n",
        "- Online Network: encoder + projector + predictor, with parameters $\\theta$;\n",
        "\n",
        "- Target Network: encoder + projector, with parameters $\\xi$.\n",
        "\n",
        "During the training, the online network is updated by the **loss function** while the target network is updated slowly by the **exponential moving average** (EMA) of the online network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Env Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# env could be local or colab\n",
        "# env = 'colab'\n",
        "env = 'local'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu4N4ymfuVgt"
      },
      "source": [
        "## Google Mount\n",
        "\n",
        "Mount to Google Drive to access my data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j67MWIrCqB0k",
        "outputId": "f27d9114-7d6c-418c-cce9-925973c43ff7"
      },
      "outputs": [],
      "source": [
        "if env == 'colab':\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ANKwmWOMMjl"
      },
      "source": [
        "## Global Area\n",
        "\n",
        "This contains import and global definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osQxV7PKuxFi"
      },
      "source": [
        "### Import Declearation\n",
        "\n",
        "Download any needed libs and import them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxwPqvR9vFuQ",
        "outputId": "b30fe642-1215-410b-a5ce-de7869526a2a"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n",
        "!pip install pydicom\n",
        "!pip install beartype\n",
        "!pip install torchlars\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b683DD1gua1C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import json\n",
        "import timm\n",
        "import torch\n",
        "import random\n",
        "import pydicom\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from PIL import Image\n",
        "import multiprocessing\n",
        "from torch import optim\n",
        "from pathlib import Path\n",
        "# from torchlars import LARS\n",
        "from functools import wraps\n",
        "from torch.nn import Module\n",
        "from beartype import beartype\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.distributed as dist\n",
        "from torch.nn import SyncBatchNorm\n",
        "from accelerate import Accelerator\n",
        "from beartype.typing import Optional\n",
        "from torchvision import transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53SBzCwZMXo9"
      },
      "source": [
        "### Global Definition\n",
        "\n",
        "This part concludes all the global definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsTfFQWkzfXo"
      },
      "outputs": [],
      "source": [
        "path_data_folder = '/content/drive/MyDrive/master-thesis/data/fetched_data_by_type'\n",
        "path_dict_map = '/content/drive/MyDrive/master-thesis/data/fetched_data_by_type/all_address_dict.json'\n",
        "path_dict_map_4dct_last_phase = '/content/drive/MyDrive/master-thesis/data/imgsnarr_3dct_last_phase/all_address_dict_4dct_last_phase.json'\n",
        "path_3dct_last_phase_folder = '/content/drive/MyDrive/master-thesis/data/imgsnarr_3dct_last_phase'\n",
        "path_3dct_last_phase_padded_zeros_mean = '/content/drive/MyDrive/master-thesis/data/imgsnarr_3dct_last_phase/padded_zeros_mean.npy'\n",
        "path_3dct_last_phase_padded_mean_mean = '/content/drive/MyDrive/master-thesis/data/imgsnarr_3dct_last_phase/padded_mean_mean.npy'\n",
        "path_3dct_last_phase_padded_zeros_std = '/content/drive/MyDrive/master-thesis/data/imgsnarr_3dct_last_phase/padded_zeros_std.npy'\n",
        "path_3dct_last_phase_padded_mean_std = '/content/drive/MyDrive/master-thesis/data/imgsnarr_3dct_last_phase/padded_mean_std.npy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV2fdx04saql"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "MAX_EPOCHS = 300\n",
        "LR = 0.2 * (BATCH_SIZE/256)\n",
        "GLOBAL_WEIGTH_DECAY = 1.5e-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXctunwmue8w"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "**<font color=red>No need to perform if DICOM Arrary has been saved!!!</font>**\n",
        "\n",
        "**NB**: The total number of 4DCT scans is 82, however, there is one more in the `map_dict` which is formed by counting the number of slices. By checking the number of folders right up the slices, 4DCT should have 20 instead of 10, for delineation. Therefore, by checking again, there is one cbct instead of ct classified in subject 119 - '12-13-2000-NA-p4-13619'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhVl1oCkL8y4"
      },
      "source": [
        "### Read mapping File\n",
        "\n",
        "This is to read the json file which records all the subject's 4DCT images with the corresponding addresses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1WXev_LxKDq",
        "outputId": "d2fc9810-e27b-4b41-dbd7-cbb3d57e3210"
      },
      "outputs": [],
      "source": [
        "with open(path_dict_map, 'r') as f:\n",
        "    dict_map = json.load(f)\n",
        "\n",
        "subjects = list(dict_map.keys())\n",
        "print('(subjects):', subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj4jQesQMBY7"
      },
      "source": [
        "### Reform and Correct mapping\n",
        "\n",
        "Because there is one mistookly classified to be a 4DCT and it should be corrected from the dict_map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXm6uFQBJVV9",
        "outputId": "843d06c4-f88a-4d0b-82c0-174ebe64d102"
      },
      "outputs": [],
      "source": [
        "# reform the dict_map to the new real path\n",
        "for subject in subjects:\n",
        "    paths_4dct = dict_map[subject]['CT']\n",
        "    paths_4dcbct = dict_map[subject]['CBCT']\n",
        "    for i, path_4dct in enumerate(paths_4dct):\n",
        "        name_4dct = path_4dct.split('\\\\')[-1]\n",
        "        paths_4dct[i] = os.path.join(path_data_folder, subject, name_4dct)\n",
        "    dict_map[subject]['CT'] = paths_4dct\n",
        "\n",
        "    for i, path_4dcbct in enumerate(paths_4dcbct):\n",
        "        name_4dcbct = path_4dcbct.split('\\\\')[-1]\n",
        "        paths_4dcbct[i] = os.path.join(path_data_folder, subject, name_4dcbct)\n",
        "    dict_map[subject]['CBCT'] = paths_4dcbct\n",
        "print(dict_map)\n",
        "\n",
        "# correct the mistook one from subject 119\n",
        "paths_4dct_mistook_119 = dict_map[subjects[19]]['CT']\n",
        "for i, path in enumerate(paths_4dct_mistook_119):\n",
        "    name_4dct = path.split('/')[-1]\n",
        "    if name_4dct.startswith('12-13'):\n",
        "        del dict_map[subjects[19]]['CT'][i]\n",
        "        dict_map[subjects[19]]['CBCT'].append(path)\n",
        "print(dict_map[subjects[19]]['CT'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MYpltvRL7-j"
      },
      "source": [
        "### Specify Phase\n",
        "\n",
        "Choose a fixed phase, the end of the inhalation, which might be the last one among 10.\n",
        "\n",
        "Besides, since the operation [-1] is not always chooses the last phase, therefore, in the second cell, a condition has been made to ensure the last phase is chosen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UlFGCv_SoKve",
        "outputId": "8a351747-f25a-4a72-86ad-918845a601a9"
      },
      "outputs": [],
      "source": [
        "# get the folders right under the folder\n",
        "def get_subfolders(path_parent):\n",
        "   return [os.path.join(path_parent, f) for f in os.listdir(path_parent) if os.path.isdir(os.path.join(path_parent, f))]\n",
        "\n",
        "get_subfolders(dict_map[subjects[1]]['CT'][0])[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBIQwzfwJ7Pk"
      },
      "outputs": [],
      "source": [
        "# initialize a new dict_map_4dct_last_phase {subject:[folder_path]}\n",
        "dict_map_4dct_last_phase = {}\n",
        "for subject in subjects:\n",
        "    dict_map_4dct_last_phase[subject] = []\n",
        "    for path_4dct in dict_map[subject]['CT']:\n",
        "        temp = get_subfolders(path_4dct)[-1]\n",
        "        if len(os.listdir(temp)) == 1:\n",
        "            temp = get_subfolders(path_4dct)[-2]\n",
        "        dict_map_4dct_last_phase[subject].append(temp)\n",
        "print(dict_map_4dct_last_phase)\n",
        "\n",
        "# save the dict_map_4dct_last_phase\n",
        "with open(path_dict_map_4dct_last_phase, 'w') as f:\n",
        "    json.dump(dict_map_4dct_last_phase, f)\n",
        "\n",
        "# compute the total number of 4dct\n",
        "total_4dct = 0\n",
        "for subject in subjects:\n",
        "    total_4dct += len(dict_map_4dct_last_phase[subject])\n",
        "print(total_4dct, dict_map_4dct_last_phase[subjects[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofe9e81idHDF"
      },
      "source": [
        "### Collect DICOM Files\n",
        "\n",
        "By maintaining a dict_map_4dct_last_phase, all the corresponding 3DCT (H, W, Slice) images have been loaded with respect to the subjects, and so that each subject could get the corresponding 3DCT images by reading the dcm files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZwl8rDTdSwo"
      },
      "outputs": [],
      "source": [
        "def collect_and_convert_dcm_files(path_folder):\n",
        "    dcm_files = [os.path.join(path_folder, f) for f in os.listdir(path_folder) if f.endswith('.dcm')]\n",
        "    slices = [pydicom.dcmread(dcm_file) for dcm_file in dcm_files]\n",
        "    slices = np.array([s.pixel_array for s in slices])\n",
        "    return dcm_files, slices.transpose(1, 2, 0)\n",
        "\n",
        "_, array_slices = collect_and_convert_dcm_files(dict_map_4dct_last_phase[subjects[4]][0])\n",
        "type(array_slices), array_slices.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8-rKh0zymU1"
      },
      "outputs": [],
      "source": [
        "# an image := 3D array (H, W, s)\n",
        "dict_3dct_last_phase = {}\n",
        "for subject in subjects:\n",
        "    dict_3dct_last_phase[subject] = []\n",
        "    for path_4dct_last_phase in dict_map_4dct_last_phase[subject]:\n",
        "        _, array_slices = collect_and_convert_dcm_files(path_4dct_last_phase)\n",
        "        image_3dct = array_slices\n",
        "        dict_3dct_last_phase[subject].append(image_3dct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h78v1lKetuK"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(path_3dct_last_phase_folder):\n",
        "    os.makedirs(path_3dct_last_phase_folder)\n",
        "    # create folder for subject\n",
        "    for subject in subjects:\n",
        "        os.makedirs(os.path.join(path_3dct_last_phase_folder, subject))\n",
        "    print('Last Phase 3DCT Images in Array - Folders Created')\n",
        "\n",
        "# save the dict_3dct_last_phase\n",
        "for subject in subjects:\n",
        "    for i, image_3dct in enumerate(dict_3dct_last_phase[subject]):\n",
        "        np.save(os.path.join(path_3dct_last_phase_folder, subject, f'3dct_{i}'), image_3dct)\n",
        "print('All Saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy2M3rQBegkL"
      },
      "source": [
        "## Load DICOM Array\n",
        "\n",
        "Since all the 3DCT images with the shape (H, W, Slice) for each have been saved in a json file by the code above, therefore, load them directly into a dict_3dct_last_phase `{subject: [(3dct image)]}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4OY2xNrey6v"
      },
      "outputs": [],
      "source": [
        "# load the dict_map_4dct_last_phase\n",
        "with open(path_dict_map_4dct_last_phase, 'r') as f:\n",
        "    dict_map_4dct_last_phase = json.load(f)\n",
        "\n",
        "subjects = list(dict_map_4dct_last_phase.keys())\n",
        "\n",
        "# Load npy files and compose them into the same variable dict_3dct_last_phase\n",
        "dict_3dct_last_phase = {}\n",
        "for subject in subjects:\n",
        "    dict_3dct_last_phase[subject] = []\n",
        "    for i in range(len(dict_map_4dct_last_phase[subject])):\n",
        "        dict_3dct_last_phase[subject].append(np.load(os.path.join(path_3dct_last_phase_folder, subject, f'3dct_{i}.npy')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s08AmYG124zl"
      },
      "source": [
        "## Registered?\n",
        "\n",
        "To see whether the slices are registered for same patient or for all of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC4fl2ZR3oZk"
      },
      "source": [
        "### Inter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9l58SDSP3C9X",
        "outputId": "aac52878-548b-4cd5-caa5-57c6c2edc7f2"
      },
      "outputs": [],
      "source": [
        "print(dict_3dct_last_phase[subjects[0]][0].shape) # only 1 ct for subject 0\n",
        "for i in range(dict_3dct_last_phase[subjects[0]][0].shape[2]):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(dict_3dct_last_phase[subjects[0]][0][:, :, i], cmap=plt.cm.bone)\n",
        "    plt.title(f\"{subjects[0].split('_')[0]} - slice {i}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gSdl7XSU8fh2",
        "outputId": "903e4037-bf0a-4e43-9ccf-d841f84f4526"
      },
      "outputs": [],
      "source": [
        "print(dict_3dct_last_phase[subjects[1]][0].shape) # only 1 ct for subject 1\n",
        "for i in range(dict_3dct_last_phase[subjects[1]][0].shape[2]):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(dict_3dct_last_phase[subjects[1]][0][:, :, i], cmap=plt.cm.bone)\n",
        "    plt.title(f\"{subjects[1].split('_')[0]} - slice {i}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7t2WUhQ95jo"
      },
      "source": [
        "### Intra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hb8ohvdi80ll",
        "outputId": "a5eb07c7-7143-4ee0-96b5-9a70c401e178"
      },
      "outputs": [],
      "source": [
        "print('num of ct for sub.19', len(dict_3dct_last_phase[subjects[19]]))\n",
        "for image_3dct in dict_3dct_last_phase[subjects[19]]:\n",
        "    print(image_3dct.shape)\n",
        "\n",
        "# first ct of subject 19\n",
        "for i in range(dict_3dct_last_phase[subjects[19]][0].shape[2]):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(dict_3dct_last_phase[subjects[19]][0][:, :, i], cmap=plt.cm.bone)\n",
        "    plt.title(f\"{subjects[19].split('_')[0]} - slice {i}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZX2BMMPK-1Ag",
        "outputId": "d7d0b630-4f3b-494c-ab81-a169634a4880"
      },
      "outputs": [],
      "source": [
        "for i in range(dict_3dct_last_phase[subjects[19]][1].shape[2]):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(dict_3dct_last_phase[subjects[19]][1][:, :, i], cmap=plt.cm.bone)\n",
        "    plt.title(f\"{subjects[19].split('_')[0]} - slice {i}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IYWdM139Bi24",
        "outputId": "9e31f9d1-ef28-41c3-a09c-f5205951d8e0"
      },
      "outputs": [],
      "source": [
        "for i in range(dict_3dct_last_phase[subjects[19]][2].shape[2]):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(dict_3dct_last_phase[subjects[19]][2][:, :, i], cmap=plt.cm.bone)\n",
        "    plt.title(f\"{subjects[19].split('_')[0]} - slice {i}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZlHKkituiNd"
      },
      "source": [
        "## Data Preprocess\n",
        "\n",
        "Aim is to compose all the data in the format of `(82, 512, 512, #slice)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LXDdMMTNQVk",
        "outputId": "16fe6663-fafd-4a97-e9ce-51709bd52bdb"
      },
      "outputs": [],
      "source": [
        "# find the max_num_slices among all 3dct\n",
        "max_num_slices = 0\n",
        "for subject in subjects:\n",
        "    for image_3dct in dict_3dct_last_phase[subject]:\n",
        "        max_num_slices = max(max_num_slices, image_3dct.shape[2])\n",
        "print('Maximum number of slices:', max_num_slices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8pxsgEMXd8-"
      },
      "source": [
        "### Fill\n",
        "\n",
        "By the data, 3DCT are different from each other along the slice dimension. Therefore, a padding technique should be considered to make them be composable to be (N, H, W, S)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjLdcm2pKQZc"
      },
      "source": [
        "#### Deprecated - First trial: Zero Padding\n",
        "\n",
        "First trial: Generate the 0 slice to fill the 3DCT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS_sPZl8qRYE"
      },
      "outputs": [],
      "source": [
        "# first trial\n",
        "# padding with 0s slices\n",
        "dict_3dct_last_phase_padded_zeros = {}\n",
        "for subject in subjects:\n",
        "    dict_3dct_last_phase_padded_zeros[subject] = []\n",
        "    for image_3dct in dict_3dct_last_phase[subject]:\n",
        "        h, w, s = image_3dct.shape\n",
        "        if s < max_num_slices:\n",
        "            padding = np.zeros((h, w, max_num_slices - s))\n",
        "            image_3dct = np.concatenate((image_3dct, padding), axis=2)\n",
        "        dict_3dct_last_phase_padded_zeros[subject].append(image_3dct)\n",
        "print(dict_3dct_last_phase_padded_zeros[subjects[0]][0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSei9Q40qvcp"
      },
      "source": [
        "#### Second Trial: Mean Padding\n",
        "\n",
        "Second trial: Generate the mean 2D slice to fill the 3DCT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ft5YK0QYQkq",
        "outputId": "d887a921-a44e-4460-fec2-2ae61950600c"
      },
      "outputs": [],
      "source": [
        "# pad the 3dct to the same size (H, W, max_num_slices) by adding mean 2d slices\n",
        "dict_map_3dct_last_phase_padded_mean = {}\n",
        "for subject in subjects:\n",
        "    dict_map_3dct_last_phase_padded_mean[subject] = []\n",
        "    for image_3dct in dict_3dct_last_phase[subject]:\n",
        "        H, W, s = image_3dct.shape\n",
        "        # pad the last dimension,\n",
        "        pad_width = ((0, 0), (0, 0), (0, max_num_slices - s)) # computation is the number of slices to pad\n",
        "        # pad with mean value of the 2d slices\n",
        "        image_3dct_padded = np.pad(image_3dct, pad_width, mode='constant', constant_values=image_3dct.mean())\n",
        "        dict_map_3dct_last_phase_padded_mean[subject].append(image_3dct_padded)\n",
        "print(dict_map_3dct_last_phase_padded_mean[subjects[0]][0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HmMss0BDjPK"
      },
      "source": [
        "visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9GH3_R_MDhyY",
        "outputId": "0202cc38-da90-45f5-9874-8187d77f7518"
      },
      "outputs": [],
      "source": [
        "for i in range(dict_map_3dct_last_phase_padded_mean[subjects[0]][0].shape[2]):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(dict_map_3dct_last_phase_padded_mean[subjects[0]][0][:, :, i], cmap=plt.cm.bone)\n",
        "    plt.title(f\"Mean Padded {subjects[0].split('_')[0]} - slice {i}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXyPc28ANae4"
      },
      "source": [
        "### Dataset - Current Version is Mean\n",
        "\n",
        "Compose and permute the dataset, and compute the mean and std for each channel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbDc-K5CMQa0"
      },
      "source": [
        "#### Dataset Composition (N, H, W, S)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6AmxW8m0_Kh",
        "outputId": "70b578f7-05d4-4a8f-ea88-ed9ec0bd6837"
      },
      "outputs": [],
      "source": [
        "dataset_3dct_last_phase_padded_mean = np.zeros((82, 512, 512, 168))\n",
        "for i, subject in enumerate(subjects):\n",
        "    for j, image_3dct in enumerate(dict_map_3dct_last_phase_padded_mean[subject]):\n",
        "        dataset_3dct_last_phase_padded_mean[i] = image_3dct\n",
        "print(dataset_3dct_last_phase_padded_mean.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQN31nSwMZIL"
      },
      "source": [
        "#### Dataset Permutation (N, S, H, W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToPyF9YUMrZk"
      },
      "outputs": [],
      "source": [
        "# free RAM\n",
        "dict_3dct_last_phase = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NPBiWqH6TOM",
        "outputId": "a667b723-c5eb-4fa3-95ce-721fed80bf05"
      },
      "outputs": [],
      "source": [
        "# recompose to be the right order (N, C, H, W)\n",
        "dataset_3dct_last_phase_padded_mean = torch.tensor(dataset_3dct_last_phase_padded_mean, dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "\n",
        "print(type(dataset_3dct_last_phase_padded_mean), dataset_3dct_last_phase_padded_mean.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owJRHyEH7Ica"
      },
      "outputs": [],
      "source": [
        "# free RAM\n",
        "dict_3dct_last_phase_padded_mean = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_JFclax4vQ"
      },
      "source": [
        "### Dataset Definition `class`\n",
        "\n",
        "Model operations work with this class to support pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFZOIEv2OVfP"
      },
      "source": [
        "#### Dataset Declearation\n",
        "\n",
        "To declear the class of the dataset which supports pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEq4LIc2NPje"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.length = data.shape[0]\n",
        "        self.num_slices = data.shape[1]\n",
        "        self.image_size = data.shape[2]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuE4IcziOCdL"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(dataset_3dct_last_phase_padded_mean)\n",
        "dataset_3dct_last_phase_padded_mean = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1sCRihFOECA"
      },
      "source": [
        "#### Dataset Split\n",
        "\n",
        "Split the training_data, and test_data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJmqSahIx4GF",
        "outputId": "6c1abf1e-742b-4213-c5cb-85dd426a2c71"
      },
      "outputs": [],
      "source": [
        "train_length = 72\n",
        "test_length = len(dataset) - train_length\n",
        "\n",
        "# random_split to split the entire dataset\n",
        "training_data, test_data = random_split(dataset, [train_length, test_length])\n",
        "\n",
        "print(\"Size (Trainset):\", len(training_data))\n",
        "print(\"Size (Testset):\", len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar3n_NOWVsJb"
      },
      "source": [
        "Apply dataloader to load them, train_dataloader and test_dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-OL7ByMTHoO"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUWp7cUHJMbZ"
      },
      "source": [
        "###### Early Stopping Strategy\n",
        "\n",
        "To employ early stopping, with the use of validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEyhAe4ZJMLh",
        "outputId": "06556e47-2696-4fea-b191-f09d2fadef57"
      },
      "outputs": [],
      "source": [
        "# define split ratio within train_dataloader for the new train and val\n",
        "split_ratio = 0.2\n",
        "print('Original dataset size ==> ', len(train_dataloader.dataset))\n",
        "train_size = int((1-split_ratio)*len(train_dataloader.dataset))\n",
        "val_size = len(train_dataloader.dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(train_dataloader.dataset, [train_size, val_size])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "print('Train size ==> ', len(train_dataset))\n",
        "print('Validation size ==> ', len(val_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtTJBhOUQGNa"
      },
      "source": [
        "#### Compute Mean and Std for Data Augment.\n",
        "\n",
        "Compute the mean and std with 168 dimensions which are for the further data augmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DhlWEkYY_Dh"
      },
      "source": [
        "##### Compute and save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H72oZHziUir1",
        "outputId": "9dfd73a2-1b9b-45b7-b4e5-71b88abd522d"
      },
      "outputs": [],
      "source": [
        "train_batch_mean = []\n",
        "train_batch_std = []\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    batch_mean = torch.mean(batch, dim=(0, 2, 3))\n",
        "    batch_std = torch.std(batch, dim=(0, 2, 3))\n",
        "    train_batch_mean.append(batch_mean)\n",
        "    train_batch_std.append(batch_std)\n",
        "\n",
        "train_mean = torch.stack(train_batch_mean).mean(dim=0)\n",
        "train_std = torch.stack(train_batch_std).mean(dim=0)\n",
        "\n",
        "print(\"Trainset Mean:\", train_mean.shape)\n",
        "print(\"Trainset Std:\", train_std.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udTjkGT4q5Hz"
      },
      "outputs": [],
      "source": [
        "np.save(path_3dct_last_phase_padded_mean_mean, train_mean.numpy())\n",
        "np.save(path_3dct_last_phase_padded_mean_std, train_std.numpy())\n",
        "\n",
        "train_batch_mean = None\n",
        "train_batch_std = None\n",
        "train_mean = None\n",
        "train_std = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlqUzK6XY5Rs"
      },
      "source": [
        "##### Load the computed Mean and Std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihY81E-JERZt"
      },
      "outputs": [],
      "source": [
        "mean = np.load(path_3dct_last_phase_padded_mean_mean)\n",
        "std = np.load(path_3dct_last_phase_padded_mean_std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdXPghL3ulk8"
      },
      "source": [
        "## Model Implementation\n",
        "\n",
        "Adjust the implementation of BYOL-PyTorch to fit the 3DCT data in my case.\n",
        "\n",
        "**Citation**: [lucidrains](https://github.com/lucidrains/byol-pytorch/) and the other one [AI Summer](https://theaisummer.com/byol/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I0EJipLzdST"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTRSYSpBzg-J"
      },
      "outputs": [],
      "source": [
        "# return the default value if the input value is None\n",
        "def default(val, def_val):\n",
        "    return def_val if val is None else val\n",
        "\n",
        "# flatten a tensor\n",
        "def flatten(t):\n",
        "    # original shape: (N, C, H, W) -> (N, C*H*W)\n",
        "    return t.reshape(t.shape[0], -1)\n",
        "\n",
        "def singleton(cache_key):\n",
        "    def inner_fn(fn): # the actual decorator, which takes the original function as input\n",
        "        @wraps(fn) # preserve the metadata of the original function\n",
        "        def wrapper(self, *args, **kwargs):\n",
        "            instance = getattr(self, cache_key) # get the instance from the cache\n",
        "            if instance is not None: # if the instance is already created, return it\n",
        "                return instance\n",
        "\n",
        "            instance = fn(self, *args, **kwargs) # create the instance\n",
        "            setattr(self, cache_key, instance) # store the instance to the cache\n",
        "            return instance\n",
        "        return wrapper\n",
        "    return inner_fn\n",
        "\n",
        "# get the device of the module\n",
        "def get_module_device(module):\n",
        "    return next(module.parameters()).device\n",
        "\n",
        "# this function is used to set the requires_grad attribute of the model\n",
        "def set_requires_grad(model, val):\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = val\n",
        "\n",
        "def MaybeSyncBatchnorm(is_distributed = None):\n",
        "    is_distributed = default(is_distributed, dist.is_initialized() and dist.get_world_size() > 1)\n",
        "    return nn.SyncBatchNorm if is_distributed else nn.BatchNorm1d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xenkAISL0FFP"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVFRlJHl0HlK"
      },
      "outputs": [],
      "source": [
        "def loss_fn(x, y):\n",
        "    # dim=-1 means the last dimension, p=2 means L2 normalization\n",
        "    x = F.normalize(x, dim=-1, p=2)\n",
        "    y = F.normalize(y, dim=-1, p=2)\n",
        "    # cosine similarity\n",
        "    return 2 - 2 * (x * y).sum(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--eL6UwF0Nj9"
      },
      "source": [
        "### <font color=red>Augmentation Utils</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQIJqIowEqoJ"
      },
      "source": [
        "#### Deprecated Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-xkxwmO0RvK"
      },
      "outputs": [],
      "source": [
        "# augmentation utils\n",
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, fn, p):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return self.fn(x)\n",
        "\n",
        "class MultiChannelColorJitter():\n",
        "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "        self.color_jitter = T.ColorJitter(brightness, contrast, saturation, hue)\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # Split the image into individual channels\n",
        "        channels = image.split(1)\n",
        "\n",
        "        # Apply color jittering to each channel\n",
        "        augmented_channels = [self.color_jitter(channel) for channel in channels]\n",
        "        # Concatenate the augmented channels along the channel dimension\n",
        "        augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "\n",
        "        return augmented_image\n",
        "\n",
        "class MultiChannelRandomGrayscale:\n",
        "    def __init__(self, p=0.1):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, image):\n",
        "        channels = image.split(1)\n",
        "        augmented_channels = [T.RandomGrayscale(p=self.p)(channel) for channel in channels]\n",
        "        augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "\n",
        "        return augmented_image\n",
        "\n",
        "class MultiChannelGaussianBlur:\n",
        "    def __init__(self, kernel_size, sigma=(0.1, 2.0)):\n",
        "        self.kernel_size = kernel_size\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, image):\n",
        "        channels = image.split(1)\n",
        "        augmented_channels = [T.GaussianBlur(self.kernel_size, self.sigma)(channel) for channel in channels]\n",
        "        augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "\n",
        "        return augmented_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1IoVoDQEvFy"
      },
      "source": [
        "##### Deprecated nn.Module Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usCHkeM2c-JK"
      },
      "outputs": [],
      "source": [
        "# augmentation utils\n",
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, fn, p):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return self.fn(x)\n",
        "\n",
        "# each augmentation follow is:\n",
        "    # 1. split the image into individual channels, my case is 168 channels (H, W, 168)\n",
        "    # 2. apply the augmentation to each channel\n",
        "    # 3. concatenate the augmented channels along the channel dimension\n",
        "# the final output is the augmented image with the same shape (H, W, 168)\n",
        "\n",
        "class MultiChannelColorJitter(nn.Module):\n",
        "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "        super().__init__()\n",
        "        self.color_jitter = T.ColorJitter(brightness, contrast, saturation, hue)\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Split the image into individual channels\n",
        "        channels = image.split(1)\n",
        "\n",
        "        # Apply color jittering to each channel\n",
        "        augmented_channels = [self.color_jitter(channel) for channel in channels]\n",
        "        # Concatenate the augmented channels along the channel dimension\n",
        "        augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "\n",
        "        return augmented_image\n",
        "\n",
        "class MultiChannelRandomGrayscale(nn.Module):\n",
        "    def __init__(self, p=0.1):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Split the image into individual channels\n",
        "        channels = image.split(1)\n",
        "        augmented_channels = [T.RandomGrayscale(p=self.p)(channel) for channel in channels]\n",
        "        augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "\n",
        "        return augmented_image\n",
        "\n",
        "class MultiChannelGaussianBlur(nn.Module):\n",
        "    def __init__(self, kernel_size, sigma=(0.1, 2.0)):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Split the image into individual channels\n",
        "        channels = image.split(1)\n",
        "        augmented_channels = [T.GaussianBlur(self.kernel_size, self.sigma)(channel) for channel in channels]\n",
        "        augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "\n",
        "        return augmented_image\n",
        "\n",
        "class MultiChannelRandomHorizontalFlip(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Split the image into individual channels\n",
        "        channels = image.split(1)\n",
        "        augmented_channels = [T.RandomHorizontalFlip(p=self.p)(channel) for channel in channels]\n",
        "        augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "\n",
        "        return augmented_image\n",
        "\n",
        "class MultiChannelRandomResizedCrop(nn.Module):\n",
        "    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.)):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.scale = scale\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Split the image into individual channels\n",
        "        channels = image.split(1)\n",
        "        augmented_channels = [T.RandomResizedCrop(self.size, scale=self.scale, ratio=self.ratio, antialias=True)(channel) for channel in channels]\n",
        "        augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "\n",
        "        return augmented_image\n",
        "\n",
        "class MultiChannelNormalize(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def forward(self, image):\n",
        "        image = image.to(torch.float32)\n",
        "        channels = image.split(1)\n",
        "        augmented_channels = [T.Normalize(self.mean[i], self.std[i])(channel) for i, channel in enumerate(channels)]\n",
        "        augmented_image = torch.cat(augmented_channels, dim=0).to(torch.float32)\n",
        "\n",
        "        return augmented_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHMH-ajsV8Jw"
      },
      "source": [
        "#### Corrected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCidC12vUHTu"
      },
      "outputs": [],
      "source": [
        "# augmentation utils\n",
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, fn, p):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return self.fn(x)\n",
        "\n",
        "# the input seems to be x: (N, C, H, W)\n",
        "class MultiChannelColorJitter(nn.Module):\n",
        "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "        super().__init__()\n",
        "        self.color_jitter = T.ColorJitter(brightness, contrast, saturation, hue)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x := (N, C, H, W)\n",
        "        N = x.shape[0]\n",
        "        augmentated_x = torch.zeros_like(x)\n",
        "        for i in range(N):\n",
        "            # Split the image into individual channels\n",
        "            channels = x[i].split(1)\n",
        "            # Apply color jittering to each channel\n",
        "            augmented_channels = [self.color_jitter(channel) for channel in channels]\n",
        "            # Concatenate the augmented channels along the channel dimension\n",
        "            augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "            augmentated_x[i] = augmented_image\n",
        "        return augmentated_x\n",
        "\n",
        "class MultiChannelRandomGrayscale(nn.Module):\n",
        "    def __init__(self, p=0.1):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x := (N, C, H, W)\n",
        "        N = x.shape[0]\n",
        "        augmentated_x = torch.zeros_like(x)\n",
        "        for i in range(N):\n",
        "            # Split the image into individual channels\n",
        "            channels = x[i].split(1)\n",
        "            augmented_channels = [T.RandomGrayscale(p=self.p)(channel) for channel in channels]\n",
        "            augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "            augmentated_x[i] = augmented_image\n",
        "        return augmentated_x\n",
        "\n",
        "class MultiChannelRandomHorizontalFlip(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x := (N, C, H, W)\n",
        "        N = x.shape[0]\n",
        "        augmentated_x = torch.zeros_like(x)\n",
        "        for i in range(N):\n",
        "            # Split the image into individual channels\n",
        "            channels = x[i].split(1)\n",
        "            augmented_channels = [T.RandomHorizontalFlip(p=self.p)(channel) for channel in channels]\n",
        "            augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "            augmentated_x[i] = augmented_image\n",
        "        return augmentated_x\n",
        "\n",
        "class MultiChannelGaussianBlur(nn.Module):\n",
        "    def __init__(self, kernel_size, sigma=(0.1, 2.0)):\n",
        "        super().__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x := (N, C, H, W)\n",
        "        N = x.shape[0]\n",
        "        augmentated_x = torch.zeros_like(x)\n",
        "        for i in range(N):\n",
        "            # Split the image into individual channels\n",
        "            channels = x[i].split(1)\n",
        "            augmented_channels = [T.GaussianBlur(self.kernel_size, self.sigma)(channel) for channel in channels]\n",
        "            augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "            augmentated_x[i] = augmented_image\n",
        "        return augmentated_x\n",
        "\n",
        "class MultiChannelRandomResizedCrop(nn.Module):\n",
        "    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.)):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.scale = scale\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x := (N, C, H, W)\n",
        "        N = x.shape[0]\n",
        "        augmentated_x = torch.zeros_like(x)\n",
        "        for i in range(N):\n",
        "            # Split the image into individual channels\n",
        "            channels = x[i].split(1)\n",
        "            augmented_channels = [T.RandomResizedCrop(self.size, scale=self.scale, ratio=self.ratio, antialias=True)(channel) for channel in channels]\n",
        "            augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "            augmentated_x[i] = augmented_image\n",
        "        return augmentated_x\n",
        "\n",
        "class MultiChannelNormalize(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super().__init__()\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x := (N, C, H, W)\n",
        "        N = x.shape[0]\n",
        "        augmentated_x = torch.zeros_like(x)\n",
        "        for i in range(N):\n",
        "            x[i] = x[i].to(torch.float32)\n",
        "            # Split the image into individual channels\n",
        "            channels = x[i].split(1)\n",
        "            augmented_channels = [T.Normalize(self.mean[i], self.std[i])(channel) for i, channel in enumerate(channels)]\n",
        "            augmented_image = torch.cat(augmented_channels, dim=0)\n",
        "            augmentated_x[i] = augmented_image\n",
        "        return augmentated_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH7miYBD1B1G"
      },
      "source": [
        "### Exponential Moving Average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHypjxcX1Fsc"
      },
      "outputs": [],
      "source": [
        "# Exponential moving average\n",
        "class EMA():\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "\n",
        "def update_moving_average(ema_updater, ma_model, current_model):\n",
        "    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "        old_weight, up_weight = ma_params.data, current_params.data\n",
        "        ma_params.data = ema_updater.update_average(old_weight, up_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFEASFuz1Ilt"
      },
      "source": [
        "### MLP\n",
        "\n",
        "Class for projector and predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EKjZeFI1TLI"
      },
      "outputs": [],
      "source": [
        "def MLP(dim, projection_size, hidden_size=4096, sync_batchnorm=None):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim, hidden_size),\n",
        "        MaybeSyncBatchnorm(sync_batchnorm)(hidden_size),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(hidden_size, projection_size)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYOvoZ362-Oz"
      },
      "source": [
        "### NetWrapper\n",
        "\n",
        "The purpose of this class is to wrap the model and the ema_model, which means,\n",
        "make the model and the ema_model as the attributes of the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E11FctZk3BkV"
      },
      "outputs": [],
      "source": [
        "# a wrapper class for the base neural network\n",
        "# will manage the interception of the hidden layer output => interception := the output of the hidden layer\n",
        "# and pipe it into the projecter and predictor nets\n",
        "\n",
        "class NetWrapper(nn.Module):\n",
        "    def __init__(self, net, projection_size, projection_hidden_size, layer=-2, sync_batchnorm=None):\n",
        "        # layer: the layer to extract the feature, -2 means the last layer, -1 means the output layer\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "        self.layer = layer\n",
        "\n",
        "        self.projector = None\n",
        "        self.projection_size = projection_size\n",
        "        self.projection_hidden_size = projection_hidden_size\n",
        "\n",
        "        self.sync_batchnorm = sync_batchnorm\n",
        "\n",
        "        self.hidden = {}\n",
        "        # register the hook to the layer, hook:=a function that is called when the hidden layer is computed\n",
        "        # the hook will store the hidden layer output to the self.hidden\n",
        "        self.hook_registered = False\n",
        "\n",
        "    # find the layer to extract the feature\n",
        "    def _find_layer(self):\n",
        "        # if the layer is a string, find the layer by name\n",
        "        if type(self.layer) == str:\n",
        "            modules = dict([*self.net.named_modules()])\n",
        "            return modules.get(self.layer, None)\n",
        "        # if the layer is an integer, find the layer by index\n",
        "        elif type(self.layer) == int:\n",
        "            children = [*self.net.children()]\n",
        "            return children[self.layer]\n",
        "        return None\n",
        "\n",
        "    # the hook function to store the hidden layer output\n",
        "    def _hook(self, _, input, output):\n",
        "        device = input[0].device\n",
        "        # store the hidden layer output\n",
        "        self.hidden[device] = flatten(output)\n",
        "\n",
        "    def _register_hook(self):\n",
        "        layer = self._find_layer()\n",
        "        assert layer is not None, f'hidden layer ({self.layer}) not found'\n",
        "        handle = layer.register_forward_hook(self._hook)\n",
        "        self.hook_registered = True\n",
        "\n",
        "    @singleton('projector') # create the projector only once\n",
        "    def _get_projector(self, hidden):\n",
        "        _, dim = hidden.shape\n",
        "        create_mlp_fn = MLP\n",
        "        projector = create_mlp_fn(dim, self.projection_size, self.projection_hidden_size, sync_batchnorm=self.sync_batchnorm)\n",
        "        return projector.to(hidden)\n",
        "\n",
        "    def get_representation(self, x):\n",
        "        if self.layer == -1:\n",
        "            return self.net(x)\n",
        "\n",
        "        # register the hook => store the hidden layer output\n",
        "        if not self.hook_registered:\n",
        "            self._register_hook()\n",
        "\n",
        "        self.hidden.clear()\n",
        "        _ = self.net(x)\n",
        "        hidden = self.hidden[x.device]\n",
        "        self.hidden.clear()\n",
        "\n",
        "        assert hidden is not None, f'hidden layer {self.layer} never emitted an output'\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, x, return_projection = True):\n",
        "        representation = self.get_representation(x)\n",
        "\n",
        "        if not return_projection:\n",
        "            return representation\n",
        "\n",
        "        projector = self._get_projector(representation)\n",
        "        projection = projector(representation)\n",
        "        return projection, representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z09wyDez67Ph"
      },
      "source": [
        "### Main BYOL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpA4nFTyRAuc"
      },
      "outputs": [],
      "source": [
        "class BYOL(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            net,\n",
        "            image_size,\n",
        "            hidden_layer = -2,\n",
        "            projection_size = 256,\n",
        "            projection_hidden_size = 4096,\n",
        "            augment_fn = None,\n",
        "            augment_fn2 = None,\n",
        "            moving_average_decay = 0.99,\n",
        "            use_momentum = True,\n",
        "            sync_batchnorm = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.net = net\n",
        "\n",
        "        # default SimCLR augmentation\n",
        "        DEFAULT_AUG = torch.nn.Sequential(\n",
        "            RandomApply(\n",
        "                MultiChannelColorJitter(0.8, 0.8, 0.8, 0.2),\n",
        "                p = 0.3\n",
        "            ),\n",
        "            MultiChannelRandomGrayscale(p=0.2),\n",
        "            MultiChannelRandomHorizontalFlip(),\n",
        "            RandomApply(\n",
        "                MultiChannelGaussianBlur((3, 3), (1.0, 2.0)),\n",
        "                p = 0.2\n",
        "            ),\n",
        "            MultiChannelRandomResizedCrop((image_size, image_size)),\n",
        "            MultiChannelNormalize(\n",
        "                mean=torch.tensor(mean),\n",
        "                std=torch.tensor(std)),\n",
        "        )\n",
        "\n",
        "        self.augment1 = default(augment_fn, DEFAULT_AUG)\n",
        "        self.augment2 = default(augment_fn2, self.augment1)\n",
        "\n",
        "        self.online_encoder = NetWrapper(\n",
        "            net,\n",
        "            projection_size,\n",
        "            projection_hidden_size,\n",
        "            layer = hidden_layer,\n",
        "            sync_batchnorm = sync_batchnorm\n",
        "        )\n",
        "\n",
        "        self.use_momentum = use_momentum\n",
        "        self.target_encoder = None\n",
        "        self.target_ema_updater = EMA(moving_average_decay)\n",
        "\n",
        "        self.online_predictor = MLP(projection_size, projection_size, projection_hidden_size)\n",
        "\n",
        "        # get device of network and make wrapper same device\n",
        "        device = get_module_device(net)\n",
        "        self.to(device)\n",
        "\n",
        "        # send a mock image tensor to instantiate singleton parameters, my data is (512, 512, 168)\n",
        "        self.forward(torch.randn(2, 168, image_size, image_size, device=device))\n",
        "\n",
        "    @singleton('target_encoder')\n",
        "    def _get_target_encoder(self):\n",
        "        target_encoder = copy.deepcopy(self.online_encoder)\n",
        "        set_requires_grad(target_encoder, False)\n",
        "        return target_encoder\n",
        "\n",
        "    def reset_moving_average(self):\n",
        "        del self.target_encoder\n",
        "        self.target_encoder = None\n",
        "\n",
        "    def update_moving_average(self):\n",
        "        assert self.use_momentum, 'you do not need to update the moving average, since you have turned off momentum for the target encoder'\n",
        "        assert self.target_encoder is not None, 'target encoder has not been created yet'\n",
        "        update_moving_average(self.target_ema_updater, self.target_encoder, self.online_encoder)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x,\n",
        "            return_embedding = False,\n",
        "            return_projection = True\n",
        "    ):\n",
        "        assert not (self.training and x.shape[0] == 1), 'you must have greater than 1 sample when training, due to the batchnorm in the projection layer'\n",
        "\n",
        "        if return_embedding:\n",
        "            return self.online_encoder(x, return_projection = return_projection)\n",
        "\n",
        "        image_one, image_two = self.augment1(x), self.augment2(x)\n",
        "\n",
        "        images = torch.cat([image_one, image_two], dim=0)\n",
        "\n",
        "        online_projections, _ = self.online_encoder(images)\n",
        "        online_predictions = self.online_predictor(online_projections)\n",
        "\n",
        "        online_pred_one, online_pred_two = online_predictions.chunk(2, dim=0) # split the predictions into two parts\n",
        "\n",
        "        with torch.no_grad():\n",
        "            target_encoder = self._get_target_encoder() if self.use_momentum else self.online_encoder\n",
        "\n",
        "            target_projections, _ = target_encoder(images)\n",
        "            target_projections = target_projections.detach()\n",
        "\n",
        "            target_proj_one, target_proj_two = target_projections.chunk(2, dim=0)\n",
        "\n",
        "        loss_one = loss_fn(online_pred_one, target_proj_two.detach())\n",
        "        loss_two = loss_fn(online_pred_two, target_proj_one.detach())\n",
        "\n",
        "        loss = loss_one + loss_two\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Ortt5gRtGQ"
      },
      "source": [
        "## Trainer\n",
        "\n",
        "This section contains the functions to train the model and the like, work as a Trainer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp5PfcHYS0XJ"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwVitaHyS2D9"
      },
      "outputs": [],
      "source": [
        "def exists(v):\n",
        "    return v is not None\n",
        "\n",
        "def cycle(dl):\n",
        "    while True:\n",
        "        for batch in dl:\n",
        "            yield batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTUua5n5S3EU"
      },
      "source": [
        "### Commented MockDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDjiqhn8RB7F"
      },
      "outputs": [],
      "source": [
        "# class\n",
        "#class MockDataset(Dataset):\n",
        "#    def __init__(self, image_size, length):\n",
        "#        self.length = length\n",
        "#        self.image_size = image_size\n",
        "\n",
        "#    def __len__(self):\n",
        "#        return self.length\n",
        "\n",
        "#    def __getitem__(self, idx):\n",
        "#        return torch.randn(168, self.image_size, self.image_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1P2t637VJBz"
      },
      "source": [
        "### Main Trainer from Official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJbN4s2rVC_U"
      },
      "outputs": [],
      "source": [
        "class BYOLTrainer(Module):\n",
        "    @beartype\n",
        "    def __init__(\n",
        "        self,\n",
        "        net: Module,\n",
        "        *,\n",
        "        image_size: int,\n",
        "        hidden_layer: str,\n",
        "        learning_rate: float,\n",
        "        dataset: Dataset,\n",
        "        num_train_steps: int,\n",
        "        batch_size: int = 16,\n",
        "        optimizer_klass = Adam,\n",
        "        checkpoint_every: int = 1000,\n",
        "        checkpoint_folder: str = './checkpoints',\n",
        "        byol_kwargs: dict = dict(),\n",
        "        optimizer_kwargs: dict = dict(),\n",
        "        accelerator_kwargs: dict = dict(),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.accelerator = Accelerator(**accelerator_kwargs)\n",
        "\n",
        "        if dist.is_initialized() and dist.get_world_size() > 1:\n",
        "            net = SyncBatchNorm.convert_sync_batchnorm(net)\n",
        "\n",
        "        self.net = net\n",
        "\n",
        "        self.byol = BYOL(net, image_size = image_size, hidden_layer = hidden_layer, **byol_kwargs)\n",
        "\n",
        "        self.optimizer = optimizer_klass(self.byol.parameters(), lr = learning_rate, **optimizer_kwargs)\n",
        "\n",
        "        self.dataloader = DataLoader(dataset, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "        self.num_train_steps = num_train_steps\n",
        "\n",
        "        self.checkpoint_every = checkpoint_every\n",
        "        self.checkpoint_folder = Path(checkpoint_folder)\n",
        "        self.checkpoint_folder.mkdir(exist_ok = True, parents = True)\n",
        "        assert self.checkpoint_folder.is_dir()\n",
        "\n",
        "        # prepare with accelerate\n",
        "        (\n",
        "            self.byol,\n",
        "            self.optimizer,\n",
        "            self.dataloader\n",
        "        ) = self.accelerator.prepare(\n",
        "            self.byol,\n",
        "            self.optimizer,\n",
        "            self.dataloader\n",
        "        )\n",
        "\n",
        "        self.register_buffer('step', torch.tensor(0))\n",
        "\n",
        "    def wait(self):\n",
        "        return self.accelerator.wait_for_everyone()\n",
        "\n",
        "    def print(self, msg):\n",
        "        return self.accelerator.print(msg)\n",
        "\n",
        "    def forward(self):\n",
        "        step = self.step.item()\n",
        "        data_it = cycle(self.dataloader)\n",
        "\n",
        "        for _ in range(self.num_train_steps):\n",
        "            images = next(data_it)\n",
        "\n",
        "            with self.accelerator.autocast():\n",
        "                loss = self.byol(images)\n",
        "                self.accelerator.backward(loss)\n",
        "\n",
        "            self.print(f'loss: {loss.item():.3f}')\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.wait()\n",
        "\n",
        "            self.byol.update_moving_average()\n",
        "\n",
        "            self.wait()\n",
        "\n",
        "            if not (step % self.checkpoint_every) and self.accelerator.is_main_process:\n",
        "                checkpoint_num = step // self.checkpoint_every\n",
        "                checkpoint_path = self.checkpoint_folder / f'checkpoint.{checkpoint_num}.pt'\n",
        "                torch.save(self.net.state_dict(), str(checkpoint_path))\n",
        "\n",
        "            self.wait()\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        self.print('training complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MxpsHr1ocEt"
      },
      "source": [
        "## Train\n",
        "\n",
        "Use of official trainer version and the other one is self-implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryUNp4cJgW3h"
      },
      "source": [
        "### Official Trainer Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5edmN-3DRxMw",
        "outputId": "f884e4e7-15d7-46a0-d572-3508a2aa0731"
      },
      "outputs": [],
      "source": [
        "resnet50 = timm.create_model('resnet50', pretrained=True, in_chans=168)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISA2iIgpdqb1",
        "outputId": "48fd1860-c940-4373-c935-d71614832e03"
      },
      "outputs": [],
      "source": [
        "! accelerate config 'default'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnraZn_YY9J6"
      },
      "outputs": [],
      "source": [
        "#dataset = MockDataset(512, 10000)\n",
        "dataset = dataset\n",
        "trainer = BYOLTrainer(\n",
        "    resnet50,\n",
        "    dataset = dataset,\n",
        "    image_size = 512,\n",
        "    hidden_layer = 'global_pool',\n",
        "    learning_rate = 3e-4,\n",
        "    num_train_steps = 100_000,\n",
        "    batch_size = 16,\n",
        "    checkpoint_every = 1000     # improved model will be saved periodically to ./checkpoints folder\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ao7Hik0AfIzs"
      },
      "outputs": [],
      "source": [
        "accelerator = Accelerator()\n",
        "trainer = accelerator.prepare(trainer)\n",
        "trainer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lZIPDFlghGO"
      },
      "source": [
        "### Self-Implemented Train Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaGFQACDltYC"
      },
      "outputs": [],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU_a-lIy_z6N"
      },
      "source": [
        "#### Without Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugre-ylWlsvq"
      },
      "outputs": [],
      "source": [
        "resnet = (timm.create_model('resnet50', pretrained=False, in_chans=168)).to(device)\n",
        "byol = (BYOL(resnet, image_size=512, hidden_layer='global_pool')).to(device)\n",
        "#resnet = (timm.create_model('resnet50', pretrained=False, in_chans=168))\n",
        "#byol = (BYOL(resnet, image_size=512, hidden_layer='global_pool'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVKjSLd5gkKE"
      },
      "outputs": [],
      "source": [
        "losses_per_epoch = []\n",
        "\n",
        "opt = torch.optim.Adam(byol.parameters(), lr=3e-4)\n",
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    losses = []\n",
        "    running_loss = 0\n",
        "    for i, images in enumerate(train_dataloader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        loss = byol(images)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        byol.update_moving_average()\n",
        "\n",
        "    epoch_loss = sum(losses)/len(losses)\n",
        "    losses_per_epoch.append(epoch_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} / {EPOCHS}, Loss: {epoch_loss}\")\n",
        "\n",
        "print(\"Training Complete\")\n",
        "\n",
        "torch.save(resnet.state_dict(), './resnet.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyK8A1TgpgIt"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses_per_epoch, label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SezX6k4t2dgg"
      },
      "source": [
        "#### Early Stopping Version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvTydQ-cBlDe"
      },
      "outputs": [],
      "source": [
        "resnet = (timm.create_model('resnet50', pretrained=False, in_chans=168)).to(device)\n",
        "byol = (BYOL(resnet, image_size=512, hidden_layer='global_pool')).to(device)\n",
        "#resnet = (timm.create_model('resnet50', pretrained=False, in_chans=168))\n",
        "#byol = (BYOL(resnet, image_size=512, hidden_layer='global_pool'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yfN019EPM76"
      },
      "source": [
        "##### Optimizier\n",
        "\n",
        "Use LARS optimizer with a cosine decay learning rate schedule, without restarts, over 1000 epochs, with a warm-up period of 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMZLD0dnPdEy"
      },
      "outputs": [],
      "source": [
        "base_opt = optim.SGD(byol.parameters(), lr=LR, weight_decay=GLOBAL_WEIGTH_DECAY)\n",
        "opt = LARS(optimizer=base_opt, eps=1e-8, trust_coef=0.001)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=MAX_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5EwDFdJMKQe"
      },
      "source": [
        "##### Main Train Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_1C1ivv4JqG",
        "outputId": "93080ada-9f90-4566-8a97-9f07025fa52b"
      },
      "outputs": [],
      "source": [
        "best_validation_loss = float('inf')\n",
        "best_epoch = 0\n",
        "no_improvement_count = 0\n",
        "patience = 10\n",
        "min_delta = 0.001\n",
        "early_stop_epoch = 0\n",
        "\n",
        "losses_per_epoch = []\n",
        "val_losses_per_epoch = []\n",
        "\n",
        "# opt = torch.optim.Adam(byol.parameters(), lr=3e-4)\n",
        "EPOCHS = MAX_EPOCHS\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    losses = []\n",
        "\n",
        "    for i, images in enumerate(train_dataloader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        loss = byol(images)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        byol.update_moving_average()\n",
        "\n",
        "    epoch_loss = sum(losses)/len(losses)\n",
        "    losses_per_epoch.append(epoch_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} / {EPOCHS}, Loss: {epoch_loss}\")\n",
        "\n",
        "    # validation step\n",
        "    val_losses = []\n",
        "    # disable gradient update\n",
        "    with torch.no_grad():\n",
        "        for j, val_images in enumerate(val_dataloader):\n",
        "            val_images = val_images.to(device)\n",
        "\n",
        "            val_loss = byol(val_images)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "    val_loss_per_epoch = sum(val_losses)/len(val_losses)\n",
        "    val_losses_per_epoch.append(val_loss_per_epoch)\n",
        "    print(f'Epoch {epoch + 1} / {EPOCHS}, Val Loss: {val_loss_per_epoch}')\n",
        "\n",
        "    # update lr scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    if val_loss_per_epoch < best_validation_loss - min_delta:\n",
        "        best_validation_loss = val_loss_per_epoch\n",
        "        best_epoch = epoch + 1 # record the best one for now\n",
        "        no_improvement_count = 0\n",
        "        # save the best model\n",
        "        torch.save(resnet.state_dict(), '/content/drive/MyDrive/master-thesis/byol-results/resnet.pt')\n",
        "    else:\n",
        "        no_improvement_count += 1\n",
        "\n",
        "    if no_improvement_count >= patience:\n",
        "        print(\"Early stopping at epoch: \", epoch+1)\n",
        "        early_stop_epoch = epoch + 1\n",
        "        break\n",
        "\n",
        "print(\"Training Complete, Best Epoch {}\".format(best_epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "DaxKlaxv7ONH",
        "outputId": "4e1ef4f8-7ad5-487d-9421-516214c57b58"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses_per_epoch, label='Training Loss')\n",
        "plt.plot(val_losses_per_epoch, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEcNKxa0GGMs"
      },
      "source": [
        "## Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Z7JBChOwO0"
      },
      "outputs": [],
      "source": [
        "resnet = (timm.create_model('resnet50', pretrained=False, in_chans=168)).to(device)\n",
        "resnet.load_state_dict(torch.load(\"/content/drive/MyDrive/master-thesis/byol-results/resnet.pt\"))\n",
        "byol = (BYOL(resnet, image_size=512, hidden_layer='global_pool')).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z60618KOxjs"
      },
      "outputs": [],
      "source": [
        "all_embeddings = []\n",
        "\n",
        "for images in train_dataloader:\n",
        "    images = images.to(device)\n",
        "    _, embeddings = byol(images, return_embedding=True)\n",
        "    all_embeddings.append(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypwv6N7VO0cm"
      },
      "outputs": [],
      "source": [
        "first_batch = [image for image in first_batch]\n",
        "\n",
        "# è®¡ç®åµå¥åé\n",
        "embeddings = []\n",
        "imgs = torch.randn(2, 3, 512, 512)\n",
        "projection, embedding = byol(imgs, return_embedding = True)\n",
        "\n",
        "# embeddings åè¡¨ä¸­ç°å¨åå«äºç¬¬ä¸ä¸ªæ¹æ¬¡ä¸­æ¯å¼ å¾åçåµå¥åé\n",
        "embeddings.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vnJUKxxNxcC6",
        "53SBzCwZMXo9",
        "XXctunwmue8w",
        "FhVl1oCkL8y4",
        "tj4jQesQMBY7",
        "8MYpltvRL7-j",
        "Ofe9e81idHDF",
        "PjLdcm2pKQZc",
        "BbDc-K5CMQa0",
        "zQN31nSwMZIL",
        "PFZOIEv2OVfP",
        "8I0EJipLzdST",
        "IQIJqIowEqoJ",
        "f1IoVoDQEvFy",
        "WHMH-ajsV8Jw",
        "BH7miYBD1B1G",
        "JFEASFuz1Ilt",
        "eYOvoZ362-Oz",
        "Z09wyDez67Ph",
        "40Ortt5gRtGQ",
        "Jp5PfcHYS0XJ",
        "UTUua5n5S3EU",
        "f1P2t637VJBz",
        "ryUNp4cJgW3h",
        "GU_a-lIy_z6N",
        "WEcNKxa0GGMs"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
