{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVFdiclPmxNH"
      },
      "source": [
        "# BYOL in PyTorch\n",
        "\n",
        "This is written by Ying Liu (yili@di.ku.dk) who is the student in the Department of Computer Science at KÃ¸benhavns Universitet. And the following code are based on the given code from the original paper in JAX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algorithm\n",
        "\n",
        "<div style=\"display:flex; justify-content: center; align-items: center; background: white\">\n",
        "    <img src=\"https://imgur.com/rBZRkOf.png\" style=\"width:45%; margin-right:1rem\" />\n",
        "    <img src=\"https://imgur.com/uudkfAk.png\" style=\"width:45%;\" />\n",
        "</div>\n",
        "\n",
        "There are three important components, they are **encoder** $f$ (ResNet-50), **projector** $g$ and **predictor** $q$, respectively. \n",
        "\n",
        "- Online Network: encoder + projector + predictor, with parameters $\\theta$;\n",
        "\n",
        "- Target Network: encoder + projector, with parameters $\\xi$.\n",
        "\n",
        "During the training, the online network is updated by the **loss function** while the target network is updated slowly by the **exponential moving average** (EMA) of the online network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import resnet50\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn6bJXaimg1D"
      },
      "outputs": [],
      "source": [
        "HPS = dict(\n",
        "    max_steps=int(1000. * 1281167 / 4096), # 1000 epochs\n",
        "    batch_size=4096,\n",
        "    mlp_hidden_size=4096,\n",
        "    projection_size=256,\n",
        "    base_target_ema=4e-3,\n",
        "    optimizer_config=dict(\n",
        "        optimizer_name='lars',\n",
        "        beta=0.9,\n",
        "        trust_coef=1e-3,\n",
        "        weight_decay=1.5e-6,\n",
        "        # As in SimCLR and official implementation of LARS, we exclude bias\n",
        "        # and batchnorm weight from the Lars adaption and weightdecay.\n",
        "        exclude_bias_from_adaption=True),\n",
        "    learnng_rate_schedule=dict(\n",
        "        # The learning rate is linearly increase up to \n",
        "        # its base value * batchsize / 256 after warmup_steps\n",
        "        # global steps and then anneal with a cosine schedule.\n",
        "        base_learning_rate=0.2,\n",
        "        warmup_steps=int(10. * 1281167 / 4096),\n",
        "        anneal_schedule='cosine'),\n",
        "    batchnorm_kwargs=dict(\n",
        "        decay_rate=0.9,\n",
        "        eps=1e-5),\n",
        "    seed=1337,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function\n",
        "\n",
        "Define the loss function in advance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(x, y):\n",
        "    x = F.normalize(x, dim=-1, p=2)\n",
        "    y = F.normalize(y, dim=-1, p=2)\n",
        "    return 2 - 2 * (x * y).sum(dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Augmentation\n",
        "\n",
        "As the same as in SimCLR. The random augmentation is used to generate two views of the same image. The random augmentation is composed of the following operations:\n",
        "\n",
        "A **random** patch of the image is selected and **resized** to 224x224 with a **random horizontal flip**, followed by a **color distortion**, consisting of a **random sequence of brightness, contrast, saturation, hue adjustments**, and an optional grayscale conversion. Finally, **Gaussian blur and solarization** are applied to the patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, fn, p):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.p = p\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return self.fn(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEFAULT_AUG\n",
        "DEFAULT_AUG = torch.nn.Sequential(\n",
        "            RandomApply(\n",
        "                T.ColorJitter(0.8, 0.8, 0.8, 0.2),\n",
        "                p = 0.3\n",
        "            ),\n",
        "            T.RandomGrayscale(p=0.2),\n",
        "            T.RandomHorizontalFlip(),\n",
        "            RandomApply(\n",
        "                T.GaussianBlur((3, 3), (1.0, 2.0)),\n",
        "                p = 0.2\n",
        "            ),\n",
        "            T.RandomResizedCrop((image_size, image_size)),\n",
        "            T.Normalize(\n",
        "                mean=torch.tensor([0.485, 0.456, 0.406]),\n",
        "                std=torch.tensor([0.229, 0.224, 0.225])),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Implementation\n",
        "\n",
        "Below are the cells for defining the **encoder**, **projector**, **predictor**, and the **loss function**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder\n",
        "\n",
        "The encoder is a ResNetV1_50x1 model with the last fully connected layer removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BYOLEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BYOLEncoder, self).__init__()\n",
        "        self.resnet = resnet50(pretrained=False)\n",
        "        self.encoder = nn.Sequential(*list(self.resnet.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MLP(dim, hidden_size, projection_size):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim, hidden_size),\n",
        "        nn.BatchNorm1d(hidden_size, **HPS['batchnorm_kwargs']),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(hidden_size, projection_size)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class network(nn.Module):\n",
        "    def __init__(self, encoder, projector):\n",
        "        super(network, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.projector = projector\n",
        "        self.predictor = projector\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using mps device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# count the number of files within the directory\n",
        "import os\n",
        "import os.path\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "# Get the number of files in the directory\n",
        "def get_num_files(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        return 0\n",
        "    return sum([len(files) for r, d, files in os.walk(directory)])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
